{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-03T06:00:35.956348Z","iopub.status.busy":"2024-07-03T06:00:35.955987Z","iopub.status.idle":"2024-07-03T06:00:35.967754Z","shell.execute_reply":"2024-07-03T06:00:35.966823Z","shell.execute_reply.started":"2024-07-03T06:00:35.956319Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T06:00:35.977087Z","iopub.status.busy":"2024-07-03T06:00:35.976766Z","iopub.status.idle":"2024-07-03T06:00:35.985070Z","shell.execute_reply":"2024-07-03T06:00:35.984331Z","shell.execute_reply.started":"2024-07-03T06:00:35.977056Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from scipy.interpolate import CubicSpline\n","from scipy import signal\n","import os\n","import sklearn\n","import shutil\n","import os.path\n","import zipfile\n","from sklearn.preprocessing import MinMaxScaler"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T06:03:14.818981Z","iopub.status.busy":"2024-07-03T06:03:14.818617Z","iopub.status.idle":"2024-07-03T06:03:14.837424Z","shell.execute_reply":"2024-07-03T06:03:14.836573Z","shell.execute_reply.started":"2024-07-03T06:03:14.818951Z"},"trusted":true},"outputs":[],"source":["def windowing(data,seq_len,stride):\n","    date_time = []\n","    for i in range(0, data.shape[0] - seq_len + 1, stride):\n","        t_slice=  data[i : i + seq_len]\n","        data_time.append(t_slice)\n","    return data_time"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T06:18:47.938278Z","iopub.status.busy":"2024-07-03T06:18:47.937948Z","iopub.status.idle":"2024-07-03T06:18:47.944429Z","shell.execute_reply":"2024-07-03T06:18:47.943503Z","shell.execute_reply.started":"2024-07-03T06:18:47.938254Z"},"trusted":true},"outputs":[],"source":["def get_spectrogram(data_time,nfft,noverlap,nperseg):\n","    spectogram = []\n","    for i in range(data_time.shape[0]):\n","        f, t, specgram = signal.spectrogram( data_time[i,:],fs=250,nfft=nfft,noverlap=noverlap,nperseg=nperseg,mode=\"psd\")\n","        specgram = np.log(specgram)\n","        specgram = (specgram - np.min(specgram)) / (np.max(specgram) - np.min(specgram))\n","        spectogram.append(specgram)\n","    return spectogram"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class WGAN(keras.Model):\n","    def __init__(\n","        self,\n","        discriminator,\n","        generator,\n","        latent_dim,\n","        discriminator_extra_steps=5,\n","        gp_weight=10.0,\n","    ):\n","        super(WGAN, self).__init__()\n","        self.discriminator = discriminator\n","        self.generator = generator\n","        self.latent_dim = latent_dim\n","        self.d_steps = discriminator_extra_steps\n","        self.gp_weight = gp_weight\n","\n","    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n","        super(WGAN, self).compile()\n","        self.d_optimizer = d_optimizer\n","        self.g_optimizer = g_optimizer\n","        self.d_loss_fn = d_loss_fn\n","        self.g_loss_fn = g_loss_fn\n","\n","    def gradient_penalty(self, batch_size, real_images, fake_images):\n","        \"\"\" Calculates the gradient penalty.\n","\n","        This loss is calculated on an interpolated image\n","        and added to the discriminator loss.\n","        \"\"\"\n","        # Get the interpolated image\n","        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n","        diff = fake_images - real_images\n","        interpolated = real_images + alpha * diff\n","\n","        with tf.GradientTape() as gp_tape:\n","            gp_tape.watch(interpolated)\n","            # 1. Get the discriminator output for this interpolated image.\n","            pred = self.discriminator(interpolated, training=True)\n","\n","        # 2. Calculate the gradients w.r.t to this interpolated image.\n","        grads = gp_tape.gradient(pred, [interpolated])[0]\n","        # 3. Calculate the norm of the gradients.\n","        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n","        gp = tf.reduce_mean((norm - 1.0) ** 2)\n","        return gp\n","\n","    def train_step(self, real_images):\n","        if isinstance(real_images, tuple):\n","            real_images = real_images[0]\n","\n","        # Get the batch size\n","        batch_size = tf.shape(real_images)[0]\n","\n","        # For each batch, we are going to perform the\n","        # following steps as laid out in the original paper:\n","        # 1. Train the generator and get the generator loss\n","        # 2. Train the discriminator and get the discriminator loss\n","        # 3. Calculate the gradient penalty\n","        # 4. Multiply this gradient penalty with a constant weight factor\n","        # 5. Add the gradient penalty to the discriminator loss\n","        # 6. Return the generator and discriminator losses as a loss dictionary\n","\n","        # Train the discriminator first. The original paper recommends training\n","        # the discriminator for `x` more steps (typically 5) as compared to\n","        # one step of the generator. Here we will train it for 3 extra steps\n","        # as compared to 5 to reduce the training time.\n","        for i in range(self.d_steps):\n","            # Get the latent vector\n","            random_latent_vectors = tf.random.normal(\n","                shape=(batch_size, self.latent_dim)\n","            )\n","            with tf.GradientTape() as tape:\n","                # Generate fake images from the latent vector\n","                fake_images = self.generator(random_latent_vectors, training=True)\n","                # Get the logits for the fake images\n","                fake_logits = self.discriminator(fake_images, training=True)\n","                # Get the logits for the real images\n","                real_logits = self.discriminator(real_images, training=True)\n","\n","                # Calculate the discriminator loss using the fake and real image logits\n","                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n","                # Calculate the gradient penalty\n","                gp = self.gradient_penalty(batch_size, real_images, fake_images)\n","                # Add the gradient penalty to the original discriminator loss\n","                d_loss = d_cost + gp * self.gp_weight\n","\n","            # Get the gradients w.r.t the discriminator loss\n","            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n","            # Update the weights of the discriminator using the discriminator optimizer\n","            self.d_optimizer.apply_gradients(\n","                zip(d_gradient, self.discriminator.trainable_variables)\n","            )\n","\n","        # Train the generator\n","        # Get the latent vector\n","        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n","        with tf.GradientTape() as tape:\n","            # Generate fake images using the generator\n","            generated_images = self.generator(random_latent_vectors, training=True)\n","            # Get the discriminator logits for fake images\n","            gen_img_logits = self.discriminator(generated_images, training=True)\n","            # Calculate the generator loss\n","            g_loss = self.g_loss_fn(gen_img_logits)\n","\n","        # Get the gradients w.r.t the generator loss\n","        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n","        # Update the weights of the generator using the generator optimizer\n","        self.g_optimizer.apply_gradients(\n","            zip(gen_gradient, self.generator.trainable_variables)\n","        )\n","        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"]},{"cell_type":"markdown","metadata":{},"source":["## Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T06:55:25.897378Z","iopub.status.busy":"2024-07-03T06:55:25.896935Z","iopub.status.idle":"2024-07-03T06:55:26.435252Z","shell.execute_reply":"2024-07-03T06:55:26.434354Z","shell.execute_reply.started":"2024-07-03T06:55:25.897343Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv(\"/kaggle/input/bridge2-10files/BBD_bridge_2.csv\")\n","# df = pd.read_csv(\"/kaggle/input/bridge2-10files/VVB_bridge_2.csv\")\n","df_x = df[['x']].plot(title='x axis')\n","df_y = df[['y']].plot(title='y axis')\n","data = df[['x','y']].values\n","min_max_scaler = MinMaxScaler()\n","norm_data = min_max_scaler.fit_transform(data)\n","x = norm_data[:,0]\n","y = norm_data[:,1]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# convert data to time series data \n","seq_len = 500\n","stride = 76\n","x_time = windowing(x,seq_len,stride)\n","y_time = windowing(y,seq_len,stride)\n","x_time = np.array(x_time)\n","y_time = np.array(y_time)\n","print(x_time.shape)\n","print(y_time.shape)"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T07:11:43.979431Z","iopub.status.busy":"2024-07-03T07:11:43.978607Z","iopub.status.idle":"2024-07-03T07:11:44.022699Z","shell.execute_reply":"2024-07-03T07:11:44.021788Z","shell.execute_reply.started":"2024-07-03T07:11:43.979401Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(60, 500, 2)\n"]}],"source":["# Convert x, y data to spectrogram\n","nfft = 1000\n","noverlap = 0\n","nperseg = 500\n","x_spec = get_spectrogram(x_time,nfft,noverlap,nperseg)\n","y_spec = get_spectrogram(y_time,nfft,noverlap,nperseg)\n","x_spec = np.array(x_spec)[:,:-1,:]\n","y_spec = np.array(y_spec)[:,:-1,:]\n","xy_spec = np.concatenate([x_spec, y_spec], axis=-1)\n","print(xy_spec.shape)\n","train_data = np.expand_dims(xy_spec,axis=-1)"]},{"cell_type":"markdown","metadata":{},"source":["## Train WGAN1"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Create discriminator model\n","def make_discriminator(D,shape):\n","    input_layer = layers.Input(shape=shape)\n","\n","    x = layers.Reshape((500,2))(input_layer)\n","\n","    x = layers.Conv1D(D, 5, strides=2, padding='same')(x)\n","    x = layers.LeakyReLU(0.2)(x)\n","\n","    x = layers.Conv1D(D*2, 5, strides=2, padding='same')(x)\n","    x = layers.LeakyReLU(0.2)(x)\n","\n","    x = layers.Conv1D(D*4, 5, strides=2, padding='same')(x)\n","    x = layers.LeakyReLU(0.2)(x)\n","\n","    x = layers.Conv1D(D*8, 5, strides=2, padding='same')(x)\n","    x = layers.LeakyReLU(0.2)(x)\n","\n","    x = layers.Conv1D(D*16, 5, strides=2, padding='same')(x)\n","    x = layers.LeakyReLU(0.2)(x)\n","\n","    x = layers.Flatten()(x)\n","    x = layers.Dense(1)(x)\n","    \n","    discriminator = keras.models.Model(input_layer, x, name=\"discriminator\")\n","    return discriminator\n","\n","D = 128\n","shape = (500,2,1)\n","discriminator = make_discriminator(D,shape)\n","discriminator.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def make_generator(G,noise_dim):\n","    noise = layers.Input(shape=(noise_dim))\n","    x = layers.Dense(1 * 128 * G, use_bias=False)(noise)\n","    x = layers.Reshape((1, 128 * G))(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    x = layers.UpSampling1D(size=2)(x)\n","    x = layers.Conv1D(64 * G, 5, padding='same')(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    x = layers.UpSampling1D(size=2)(x)\n","    x = layers.Conv1D(32 * G, 5, padding='same')(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    x = layers.UpSampling1D(size=2)(x)\n","    x = layers.Conv1D(16 * G, 5, padding='same')(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    x = layers.UpSampling1D(size=2)(x)\n","    x = layers.Conv1D(8 * G, 5, padding='same')(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    x = layers.UpSampling1D(size=2)(x)\n","    x = layers.Conv1D(4 * G, 5, padding='same')(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    x = layers.UpSampling1D(size=2)(x)\n","    x = layers.Conv1D(2 * G, 5, padding='same')(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    x = layers.UpSampling1D(size=2)(x)\n","    x = layers.Conv1D(1 * G, 5, padding='same')(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    x = layers.UpSampling1D(size=2)(x)\n","    x = layers.Conv1D(G, 5, padding='same')(x)\n","    x = layers.Activation(\"relu\")(x)\n","    x = layers.Cropping1D(cropping=3)(x)\n","\n","    x = layers.UpSampling1D(size=2)(x)\n","    x = layers.Conv1D(2, 5, padding='same')(x)\n","    x = layers.Activation(\"tanh\")(x)\n","\n","    x = layers.Reshape((500,2,1))(x)\n"," \n","    generator = keras.models.Model(noise, x, name=\"generator\")\n","    return generator\n","\n","noise_dim = 15\n","G = 32\n","generator = make_generator(G,noise_dim)\n","generator.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["os.mkdir('/kaggle/working/loss_graph')\n","os.mkdir('/kaggle/working/spec_gen')\n","os.mkdir('/kaggle/working/disc_gen')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-27T03:55:18.087322Z","iopub.status.busy":"2023-10-27T03:55:18.086906Z","iopub.status.idle":"2023-10-27T03:56:45.715510Z","shell.execute_reply":"2023-10-27T03:56:45.714433Z","shell.execute_reply.started":"2023-10-27T03:55:18.087291Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["class GANMonitor(keras.callbacks.Callback):\n","    def __init__(self, tries, num_img=6, latent_dim=noise_dim,plot_interval=500):\n","        self.num_img = num_img\n","        self.latent_dim = latent_dim\n","        self.plot_interval = plot_interval\n","        self.d_losses = []\n","        self.g_losses = []\n","        self.tries = tries\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        self.d_losses.append(logs[\"d_loss\"])\n","        self.g_losses.append(logs[\"g_loss\"])\n","        if epoch >= 0:\n","            if epoch % self.plot_interval == 0:\n","                self.plot_losses(epoch)\n","                self.save_model(epoch)\n","\n","    def plot_losses(self, epoch):\n","        plt.figure(figsize=(10, 5))\n","        plt.plot(self.g_losses, label=\"Generator Loss\")\n","        plt.plot(self.d_losses, label=\"Discriminator Loss\")\n","        plt.xlabel(\"Epoch\")\n","        plt.ylabel(\"Loss\")\n","        plt.title(\"GAN Losses\")\n","        plt.legend()\n","        plt.savefig('/kaggle/working/loss_graph/Disc_loss_%04d_epoch%04d.png' % (self.tries, epoch))\n","\n","    def save_model(self, epoch):\n","        generator.save('/kaggle/working/spec_gen/generator_%04d_epoch%04d.keras' % (self.tries, epoch))\n","        discriminator.save('/kaggle/working/disc_gen/discriminator_%04d_epoch%04d.keras' % (self.tries, epoch))\n","\n","# define optimizers and loss function \n","generator_optimizer = keras.optimizers.Adam(\n","    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",")\n","discriminator_optimizer = keras.optimizers.Adam(\n","    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",")\n","\n","def discriminator_loss(real_img, fake_img):\n","    real_loss = tf.reduce_mean(real_img)\n","    fake_loss = tf.reduce_mean(fake_img)\n","    return fake_loss - real_loss\n","\n","def generator_loss(fake_img):\n","    return -tf.reduce_mean(fake_img)\n","\n","# Set hyperparameter\n","epochs = 25000\n","tries = 9\n","batch_size = 10\n","\n","# Instantiate callback\n","cbk = GANMonitor(tries, num_img=1, latent_dim=noise_dim,plot_interval=1000)\n","\n","# Instantiate the WGAN model.\n","wgan = WGAN(\n","    discriminator=discriminator,\n","    generator=generator,\n","    latent_dim=noise_dim,\n","    discriminator_extra_steps=5,\n",")\n","\n","# Compile the WGAN model.\n","wgan.compile(\n","    d_optimizer=discriminator_optimizer,\n","    g_optimizer=generator_optimizer,\n","    g_loss_fn=generator_loss,\n","    d_loss_fn=discriminator_loss,\n",")\n","\n","# Start training the model.\n","history = wgan.fit(train_data, batch_size=batch_size, epochs=epochs, callbacks=[cbk])\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":3841975,"sourceId":6813978,"sourceType":"datasetVersion"}],"dockerImageVersionId":30559,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
