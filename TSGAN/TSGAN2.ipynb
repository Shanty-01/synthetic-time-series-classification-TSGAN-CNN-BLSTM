{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-03T08:39:14.389016Z","iopub.status.busy":"2024-07-03T08:39:14.388426Z","iopub.status.idle":"2024-07-03T08:39:14.407869Z","shell.execute_reply":"2024-07-03T08:39:14.406692Z","shell.execute_reply.started":"2024-07-03T08:39:14.388984Z"},"papermill":{"duration":0.665675,"end_time":"2023-11-23T06:28:57.555828","exception":false,"start_time":"2023-11-23T06:28:56.890153","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T08:39:15.230025Z","iopub.status.busy":"2024-07-03T08:39:15.229585Z","iopub.status.idle":"2024-07-03T08:39:33.092167Z","shell.execute_reply":"2024-07-03T08:39:33.090807Z","shell.execute_reply.started":"2024-07-03T08:39:15.229991Z"},"papermill":{"duration":15.066966,"end_time":"2023-11-23T06:29:12.675460","exception":false,"start_time":"2023-11-23T06:28:57.608494","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["!pip install liac-arff"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T08:41:37.531347Z","iopub.status.busy":"2024-07-03T08:41:37.530376Z","iopub.status.idle":"2024-07-03T08:41:37.541699Z","shell.execute_reply":"2024-07-03T08:41:37.540585Z","shell.execute_reply.started":"2024-07-03T08:41:37.531299Z"},"papermill":{"duration":8.451377,"end_time":"2023-11-23T06:29:21.135852","exception":false,"start_time":"2023-11-23T06:29:12.684475","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from scipy.interpolate import CubicSpline\n","from scipy import signal\n","import json\n","import os\n","import sklearn\n","from arff import loads, dump\n","import shutil\n","import os.path\n","import re\n","import zipfile\n","from sklearn.preprocessing import RobustScaler,MinMaxScaler \n","from sklearn.decomposition import PCA"]},{"cell_type":"code","execution_count":null,"id":"ae527c5d","metadata":{},"outputs":[],"source":["def windowing(data,seq_len,stride):\n","    date_time = []\n","    for i in range(0, data.shape[0] - seq_len + 1, stride):\n","        t_slice=  data[i : i + seq_len]\n","        data_time.append(t_slice)\n","    return data_time"]},{"cell_type":"markdown","metadata":{},"source":["## Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T08:41:40.741723Z","iopub.status.busy":"2024-07-03T08:41:40.741196Z","iopub.status.idle":"2024-07-03T08:41:41.438698Z","shell.execute_reply":"2024-07-03T08:41:41.437594Z","shell.execute_reply.started":"2024-07-03T08:41:40.741680Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv(\"/kaggle/input/bridge2-10files/BBD_bridge_2.csv\")\n","# df = pd.read_csv(\"/kaggle/input/bridge2-10files/VVB_bridge_2.csv\")\n","df_x = df[['x']].plot(title='x axis')\n","df_y = df[['y']].plot(title='y axis')\n","data = df[['x','y']].values\n","min_max_scaler = MinMaxScaler()\n","norm_data = min_max_scaler.fit_transform(data)\n","x = norm_data[:,0]\n","y = norm_data[:,1]"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T08:43:29.344207Z","iopub.status.busy":"2024-07-03T08:43:29.343727Z","iopub.status.idle":"2024-07-03T08:43:29.353685Z","shell.execute_reply":"2024-07-03T08:43:29.352466Z","shell.execute_reply.started":"2024-07-03T08:43:29.344164Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(60, 500, 2)\n"]}],"source":["# convert data to time series data \n","seq_len = 500\n","stride = 76\n","x_time = windowing(x,seq_len,stride)\n","y_time = windowing(y,seq_len,stride)\n","x_time = np.array(x_time)\n","y_time = np.array(y_time)\n","x_time = np.expand_dims(x_time, axis=2)\n","y_time = np.expand_dims(y_time, axis=2)\n","xy_time = np.concatenate([x_time, y_time], axis=-1)\n","print(xy_time.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Train WGAN2"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T06:29:21.238196Z","iopub.status.busy":"2023-11-23T06:29:21.237922Z","iopub.status.idle":"2023-11-23T06:29:26.882252Z","shell.execute_reply":"2023-11-23T06:29:26.881278Z"},"papermill":{"duration":5.656004,"end_time":"2023-11-23T06:29:26.884752","exception":false,"start_time":"2023-11-23T06:29:21.228748","status":"completed"},"tags":[]},"outputs":[],"source":["# Define path of WGAN1 generator \n","gen_path = '/kaggle/input/generator0005/generator_0009.keras'\n","generator1 = keras.models.load_model(gen_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.167821,"end_time":"2023-11-23T06:29:27.087147","exception":false,"start_time":"2023-11-23T06:29:26.919326","status":"completed"},"tags":[]},"outputs":[],"source":["def make_discriminator2(D):\n","    input_layer = layers.Input(shape=(500,2,1))\n","\n","    x = layers.Reshape((500,2))(input_layer)\n","\n","    x = layers.Conv1D(D, 5, strides=2, padding='same')(x)\n","    x = layers.LeakyReLU(0.2)(x)\n","\n","    x = layers.Conv1D(D*2, 5, strides=2, padding='same')(x)\n","    x = layers.LeakyReLU(0.2)(x)\n","\n","    x = layers.Conv1D(D*4, 5, strides=2, padding='same')(x)\n","    x = layers.LeakyReLU(0.2)(x)\n","\n","    x = layers.Conv1D(D*8, 5, strides=2, padding='same')(x)\n","    x = layers.LeakyReLU(0.2)(x)\n","\n","    x = layers.Conv1D(D*16, 5, strides=2, padding='same')(x)\n","    x = layers.LeakyReLU(0.2)(x)\n","\n","    x = layers.Flatten()(x)\n","    x = layers.Dense(1)(x)\n","\n","    discriminator2 = keras.models.Model(input_layer, x, name=\"discriminator2\")\n","    return discriminator2\n","D = 64\n","discriminator2 = make_discriminator2(D)\n","discriminator2.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.508469,"end_time":"2023-11-23T06:29:27.606770","exception":false,"start_time":"2023-11-23T06:29:27.098301","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def make_generator2(G):\n","    noise = layers.Input(shape=(500,2,1))\n","\n","    x = layers.Reshape((500,2))(noise)\n","\n","    x = layers.Conv1D(16 * G, 10, strides=2, padding='same')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    x = layers.Conv1D(8 * G, 10, strides=2, padding='same')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    x = layers.Conv1D(4 * G, 10, strides=2, padding='same')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    x = layers.Conv1D(2 * G, 10, strides=2, padding='same')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    x = layers.Conv1D(1 , 10, strides=1, padding='same')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    x = layers.UpSampling1D(size=2)(x)\n","    x = layers.Conv1D(2 * G, 10, padding='same')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    x = layers.UpSampling1D(size=2)(x)\n","    x = layers.Conv1D(4 * G, 10, padding='same')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    x = layers.UpSampling1D(size=2)(x)\n","    x = layers.Conv1D(4 * G, 10, padding='same')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    x = layers.UpSampling1D(size=2)(x)\n","    x = layers.Conv1D(8 * G, 10, padding='same')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    x = layers.Conv1D(2, 10, padding='same')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"tanh\")(x)\n","\n","    x = layers.Cropping1D(6)(x)\n","    x = layers.Reshape((500,2,1))(x)\n","\n","    generator2 = keras.models.Model(noise, x, name=\"generator2\")\n","    return generator2\n","G = 32\n","generator2 = make_generator2(G)\n","generator2.summary()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T06:29:27.647059Z","iopub.status.busy":"2023-11-23T06:29:27.646759Z","iopub.status.idle":"2023-11-23T06:29:27.668456Z","shell.execute_reply":"2023-11-23T06:29:27.667528Z"},"papermill":{"duration":0.044144,"end_time":"2023-11-23T06:29:27.670573","exception":false,"start_time":"2023-11-23T06:29:27.626429","status":"completed"},"tags":[]},"outputs":[],"source":["class WGAN(keras.Model):\n","    def __init__(\n","        self,\n","        discriminator,\n","        generator,\n","        latent_dim,\n","        discriminator_extra_steps=5,\n","        gp_weight=10.0,\n","    ):\n","        super(WGAN, self).__init__()\n","        self.discriminator = discriminator\n","        self.generator = generator\n","        self.latent_dim = latent_dim\n","        self.d_steps = discriminator_extra_steps\n","        self.gp_weight = gp_weight\n","\n","    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n","        super(WGAN, self).compile()\n","        self.d_optimizer = d_optimizer\n","        self.g_optimizer = g_optimizer\n","        self.d_loss_fn = d_loss_fn\n","        self.g_loss_fn = g_loss_fn\n","\n","    def gradient_penalty(self, batch_size, real_images, fake_images):\n","        \"\"\" Calculates the gradient penalty.\n","\n","        This loss is calculated on an interpolated image\n","        and added to the discriminator loss.\n","        \"\"\"\n","        # Get the interpolated image\n","\n","        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n","        diff = fake_images - real_images\n","        interpolated = real_images + alpha * diff\n","\n","        with tf.GradientTape() as gp_tape:\n","            gp_tape.watch(interpolated)\n","            # 1. Get the discriminator output for this interpolated image.\n","            pred = self.discriminator(interpolated, training=True)\n","\n","        # 2. Calculate the gradients w.r.t to this interpolated image.\n","        grads = gp_tape.gradient(pred, [interpolated])[0]\n","        # 3. Calculate the norm of the gradients.\n","        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n","        gp = tf.reduce_mean((norm - 1.0) ** 2)\n","        return gp\n","\n","    def train_step(self, real_images):\n","        if isinstance(real_images, tuple):\n","            real_images = real_images[0]\n","\n","        # Get the batch size\n","        batch_size = tf.shape(real_images)[0]\n","\n","        # For each batch, we are going to perform the\n","        # following steps as laid out in the original paper:\n","        # 1. Train the generator and get the generator loss\n","        # 2. Train the discriminator and get the discriminator loss\n","        # 3. Calculate the gradient penalty\n","        # 4. Multiply this gradient penalty with a constant weight factor\n","        # 5. Add the gradient penalty to the discriminator loss\n","        # 6. Return the generator and discriminator losses as a loss dictionary\n","\n","        # Train the discriminator first. The original paper recommends training\n","        # the discriminator for `x` more steps (typically 5) as compared to\n","        # one step of the generator. Here we will train it for 3 extra steps\n","        # as compared to 5 to reduce the training time.\n","        for i in range(self.d_steps):\n","            # Get the latent vector\n","            random_latent_vectors = generator1(tf.random.normal(\n","                shape=(batch_size, self.latent_dim)), training=False)\n","            with tf.GradientTape() as tape:\n","                # Generate fake images from the latent vector\n","                fake_images = self.generator(random_latent_vectors, training=True)\n","                # Get the logits for the fake images\n","                fake_logits = self.discriminator(fake_images, training=True)\n","                # Get the logits for the real images\n","                real_logits = self.discriminator(real_images, training=True)\n","\n","                # Calculate the discriminator loss using the fake and real image logits\n","                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n","                # Calculate the gradient penalty\n","                gp = self.gradient_penalty(batch_size, real_images, fake_images)\n","                # Add the gradient penalty to the original discriminator loss\n","                d_loss = d_cost + gp * self.gp_weight\n","\n","            # Get the gradients w.r.t the discriminator loss\n","            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n","            # Update the weights of the discriminator using the discriminator optimizer\n","            self.d_optimizer.apply_gradients(\n","                zip(d_gradient, self.discriminator.trainable_variables)\n","            )\n","\n","        # Train the generator\n","        # Get the latent vector\n","        random_latent_vectors = generator1(tf.random.normal(\n","                shape=(batch_size, self.latent_dim)), training=False)\n","        with tf.GradientTape() as tape:\n","            # Generate fake images using the generator\n","            generated_images = self.generator(random_latent_vectors, training=True)\n","            # Get the discriminator logits for fake images\n","            gen_img_logits = self.discriminator(generated_images, training=True)\n","            # Calculate the generator loss\n","            g_loss = self.g_loss_fn(gen_img_logits)\n","\n","        # Get the gradients w.r.t the generator loss\n","        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n","        # Update the weights of the generator using the generator optimizer\n","        self.g_optimizer.apply_gradients(\n","            zip(gen_gradient, self.generator.trainable_variables)\n","        )\n","        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T06:29:27.708623Z","iopub.status.busy":"2023-11-23T06:29:27.708311Z","iopub.status.idle":"2023-11-23T06:29:27.722659Z","shell.execute_reply":"2023-11-23T06:29:27.721897Z"},"papermill":{"duration":0.035584,"end_time":"2023-11-23T06:29:27.724551","exception":false,"start_time":"2023-11-23T06:29:27.688967","status":"completed"},"tags":[]},"outputs":[],"source":["class GANMonitor(keras.callbacks.Callback):\n","    def __init__(self, tries, num_img=6, latent_dim=noise_dim,plot_interval=500,plot_interval_img=10):\n","        self.num_img = num_img\n","        self.latent_dim = latent_dim\n","        self.plot_interval = plot_interval\n","        self.plot_interval_img = plot_interval_img\n","        self.d_losses = []\n","        self.g_losses = []\n","        self.tries = tries\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        self.d_losses.append(logs[\"d_loss\"])\n","        self.g_losses.append(logs[\"g_loss\"])\n","        if epoch % self.plot_interval == 0:\n","            self.plot_losses(epoch)\n","            self.save_model(epoch)\n","            self.plot_synth(epoch)\n","        if epoch % self.plot_interval_img == 0:\n","            self.plot_synth_img(epoch)\n","            \n","            \n","    def plot_synth_img(self, epoch):\n","        random_latent_vectors = generator1(tf.random.normal(\n","        shape=(self.num_img, self.latent_dim)), training=False)\n","        generated_images = self.model.generator(random_latent_vectors)\n","\n","        plt.figure()\n","        for i in range(self.num_img):\n","            img = generated_images[i].numpy()\n","            plt.plot(img[:,0,0])\n","#         plt.show()\n","#         plt.savefig('/kaggle/working/synth_img/synth_%d_%04d.png' % (self.tries, epoch))\n","        plt.close()\n","    def plot_synth(self, epoch):\n","        random_latent_vectors = generator1(tf.random.normal(\n","        shape=(self.num_img, self.latent_dim)), training=False)\n","        generated_images = self.model.generator(random_latent_vectors)\n","\n","        plt.figure()\n","        for i in range(self.num_img):\n","            img = generated_images[i].numpy()\n","            plt.plot(img[:,0,0])\n","        plt.savefig('/kaggle/working/synth_img/synth_%d_%04d.png' % (self.tries, epoch))\n","        plt.close()\n","\n","    def plot_losses(self, epoch):\n","        plt.figure(figsize=(10, 5))\n","        plt.plot(self.g_losses, label=\"Generator Loss\")\n","        plt.plot(self.d_losses, label=\"Discriminator Loss\")\n","        plt.xlabel(\"Epoch\")\n","        plt.ylabel(\"Loss\")\n","        plt.title(\"GAN Losses\")\n","        plt.legend()\n","        plt.savefig('/kaggle/working/loss_graph/time_%d_loss_%04d.png' % (self.tries, epoch))\n","        plt.close()\n","    def save_model(self, epoch):\n","        generator2.save('/kaggle/working/time_gen/gen_%d_time_%04d.keras' % (self.tries, epoch))\n","        discriminator2.save('/kaggle/working/time_disc/disc_%d_time_%04d.keras' % (self.tries, epoch))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["os.mkdir('/kaggle/working/synth_img')\n","os.mkdir('/kaggle/working/time_gen')\n","os.mkdir('/kaggle/working/time_disc')\n","os.mkdir('/kaggle/working/loss_graph')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T06:29:27.764031Z","iopub.status.busy":"2023-11-23T06:29:27.763761Z","iopub.status.idle":"2023-11-23T13:36:57.574412Z","shell.execute_reply":"2023-11-23T13:36:57.573643Z"},"papermill":{"duration":25649.83273,"end_time":"2023-11-23T13:36:57.576542","exception":false,"start_time":"2023-11-23T06:29:27.743812","status":"completed"},"scrolled":true,"tags":[]},"outputs":[],"source":["# define optimizers and loss function \n","generator_optimizer = keras.optimizers.Adam(\n","    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",")\n","discriminator_optimizer = keras.optimizers.Adam(\n","    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",")\n","\n","def discriminator_loss(real_img, fake_img):\n","    real_loss = tf.reduce_mean(real_img)\n","    fake_loss = tf.reduce_mean(fake_img)\n","    return fake_loss - real_loss\n","\n","def generator_loss(fake_img):\n","    return -tf.reduce_mean(fake_img)\n","# The negative sign is used because, in optimization tasks, the goal is\n","# often to minimize a loss or cost function. By negating the score, the optimization\n","# process is turned into a maximization problem, which is a common approach when\n","# using minimization-based optimizers. This is known as minimizing the negative of\n","# the objective function.\n","## So the disc is trying to minimize fake_loss to distinguish it from the real samples\n","## and the generator is trying to maximize the fake loss to make fake data indistinguishable from real samples\n","\n","# Set hyperparameter\n","epochs = 25000\n","tries = 9\n","batch_size = 10\n","noise_dim = 15\n","train_data = np.expand_dims(xy_time, axis=3)\n","# Instantiate the customer `GANMonitor` Keras callback.\n","\n","## UNCOMMENT FOR COMPLETE CODE\n","cbk = GANMonitor(tries, num_img=5, latent_dim=noise_dim,plot_interval=1000,plot_interval_img=30)\n","\n","# Instantiate the WGAN model.\n","wgan2 = WGAN(\n","    discriminator=discriminator2,\n","    generator=generator2,\n","    latent_dim=noise_dim,\n","    discriminator_extra_steps=5,\n",")\n","\n","# Compile the WGAN model.\n","wgan2.compile(\n","    d_optimizer=discriminator_optimizer,\n","    g_optimizer=generator_optimizer,\n","    g_loss_fn=generator_loss,\n","    d_loss_fn=discriminator_loss,\n",")\n","\n","# Start training the model.\n","history = wgan2.fit(train_data, batch_size=batch_size, epochs=epochs, callbacks=[cbk])\n","\n","generator2.save('/kaggle/working/time_gen/gen_%04d.keras' % (tries))\n","discriminator2.save('/kaggle/working/time_disc/disc_%04d.keras' % (tries))"]},{"cell_type":"markdown","metadata":{},"source":["## Plot Synthetic Data"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T09:18:25.729847Z","iopub.status.busy":"2024-07-03T09:18:25.729353Z","iopub.status.idle":"2024-07-03T09:18:30.382212Z","shell.execute_reply":"2024-07-03T09:18:30.380990Z","shell.execute_reply.started":"2024-07-03T09:18:25.729816Z"},"papermill":{"duration":16.024175,"end_time":"2023-11-23T13:38:02.471041","exception":false,"start_time":"2023-11-23T13:37:46.446866","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Define path to WGAN1 and WGAN2 generator \n","gen_path = '/kaggle/input/generator0005/generator_0007.keras'\n","generator1 = keras.models.load_model(gen_path)\n","gen_path2 = '/kaggle/input/generator0005/gen_7_time_18000.keras'\n","generator2 = keras.models.load_model(gen_path2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T09:36:05.563894Z","iopub.status.busy":"2024-07-03T09:36:05.563508Z","iopub.status.idle":"2024-07-03T09:36:18.722923Z","shell.execute_reply":"2024-07-03T09:36:18.721718Z","shell.execute_reply.started":"2024-07-03T09:36:05.563863Z"},"papermill":{"duration":15.781688,"end_time":"2023-11-23T13:38:34.149787","exception":false,"start_time":"2023-11-23T13:38:18.368099","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["synth_data_x = []\n","synth_data_y = []\n","n = 100\n","for i in range(n):\n","    x = generator2(generator1(tf.random.uniform(shape=(1,noise_dim), minval=-3, maxval=3), training=False),training=False)\n","    x = x.numpy()\n","    x = min_max_scaler.inverse_transform(x[0,:,:,0])\n","    x_hat = x[:,0]\n","    y_hat = x[:,1]\n","    synth_data_x.append(x_hat)\n","    synth_data_y.append(y_hat)\n","\n","# Create a plot for synthetic X data with overlapping curves\n","plt.figure()  \n","for i in range(n):\n","    plt.plot(synth_data_x[i])\n","plt.xlabel('Time Step')\n","plt.ylabel('Value')\n","plt.title('Synthetic X Data with Overlapping Curves')\n","# plt.savefig('/kaggle/working/result/synth_x_5sample_BBD5.png')\n","\n","# Create a plot for synthetic Y data with overlapping curves\n","plt.figure()  \n","for i in range(n):\n","    plt.plot(synth_data_y[i])\n","plt.xlabel('Time Step')\n","plt.ylabel('Value')\n","plt.title('Synthetic Y Data with Overlapping Curves')\n","# plt.savefig('/kaggle/working/result/synth_y_5sample_BBD5.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T09:45:45.968688Z","iopub.status.busy":"2024-07-03T09:45:45.968285Z","iopub.status.idle":"2024-07-03T09:45:49.202124Z","shell.execute_reply":"2024-07-03T09:45:49.200837Z","shell.execute_reply.started":"2024-07-03T09:45:45.968657Z"},"trusted":true},"outputs":[],"source":["# create scatter plot\n","sample = generator2(generator1(tf.random.normal(shape=(n, 15)), training=False),training=False)\n","sample = sample.numpy()\n","synth_data = sample.reshape(-1, 2)\n","train_data = xy_time.reshape(-1, 2)\n","plt.scatter(synth_data[:,0], synth_data[:,1], alpha = 0.2, label = \"Synthetic\")\n","plt.scatter(train_data[:,0], train_data[:,1], alpha = 0.2, label = \"Original\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":16.064182,"end_time":"2023-11-23T13:40:41.996420","exception":false,"start_time":"2023-11-23T13:40:25.932238","status":"completed"},"tags":[]},"source":["## MAKE ARFF FILES"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T13:41:13.728636Z","iopub.status.busy":"2023-11-23T13:41:13.728253Z","iopub.status.idle":"2023-11-23T13:41:13.732617Z","shell.execute_reply":"2023-11-23T13:41:13.731732Z"},"papermill":{"duration":15.953742,"end_time":"2023-11-23T13:41:13.734507","exception":false,"start_time":"2023-11-23T13:40:57.780765","status":"completed"},"tags":[]},"outputs":[],"source":["os.mkdir('/kaggle/working/fid')"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T13:42:49.850295Z","iopub.status.busy":"2023-11-23T13:42:49.849522Z","iopub.status.idle":"2023-11-23T13:42:49.853755Z","shell.execute_reply":"2023-11-23T13:42:49.852839Z"},"papermill":{"duration":15.8841,"end_time":"2023-11-23T13:42:49.855634","exception":false,"start_time":"2023-11-23T13:42:33.971534","status":"completed"},"tags":[]},"outputs":[],"source":["n = 0\n","n_files = 1\n","batch_num = 10\n","noise_dim = 15"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T13:43:21.733129Z","iopub.status.busy":"2023-11-23T13:43:21.732782Z","iopub.status.idle":"2023-11-23T13:43:21.739439Z","shell.execute_reply":"2023-11-23T13:43:21.738457Z"},"papermill":{"duration":15.670585,"end_time":"2023-11-23T13:43:21.741459","exception":false,"start_time":"2023-11-23T13:43:06.070874","status":"completed"},"tags":[]},"outputs":[],"source":["for i in range(n_files):\n","    n = 502\n","    xy_hat = generator2(generator1(tf.random.normal(shape=(n, 15)), training=False),training=False)\n","    xy_hat = xy_hat.numpy()\n","    xy_hat[:,:,0,0] = (xy_hat[:,:,0,0]+1)*(max_x-min_x)/2 + min_x\n","    xy_hat[:,:,1,0] = (xy_hat[:,:,1,0]+1)*(max_y-min_y)/2 + min_y\n","    xy_hat = np.squeeze(xy_hat)\n","    xy_hat = np.concatenate(xy_hat, axis=0)\n","    x_hat = xy_hat[:, 0]\n","    y_hat = xy_hat[:, 1]\n","    x_hat = np.expand_dims(x_hat, axis=1)\n","    y_hat = np.expand_dims(y_hat, axis=1)\n","    time = np.array([range(0,x_hat.shape[0])])\n","    time = np.squeeze(time)\n","    time = np.expand_dims(time, axis=1)\n","    time = time.astype(int)\n","    confidence = np.ones(time.shape[0])\n","    confidence = np.expand_dims(confidence, axis=1)\n","    confidence = confidence.astype(int)\n","    synth_dataset = np.hstack((time, x_hat, y_hat, confidence))\n","\n","    data = synth_dataset\n","\n","    # Define attributes\n","    attributes = [\n","      ('time', 'INTEGER'),\n","      ('x', 'NUMERIC'),\n","      ('y', 'NUMERIC'),\n","      ('confidence', 'NUMERIC')\n","    ]\n","\n","    # Create ARFF dataset\n","    dataset = {\n","      'data': data,\n","      'attributes': attributes,\n","      'relation': 'synth_gaze',\n","      'description': (\n","          ' Handlabelling data is arranged in 3 columns: handlabeller1 and handlabeller2 indicate the first two experts,'\n","          ' handlabeller_final column contains the final, tie-breaking labels.'\n","          '\\n Labels in these columns are to be interpreted as follows:'\n","          '\\n   - 0 is UNKNOWN'\n","          '\\n   - 1 is FIX (fixation)'\n","          '\\n   - 2 is SACCADE'\n","          '\\n   - 3 is SP (smooth pursuit)'\n","          '\\n   - 4 is NOISE'\n","          '\\n'\n","          '\\n@METADATA width_px 1280.0'\n","          '\\n@METADATA height_px 720.0'\n","          '\\n@METADATA width_mm 400.0'\n","          '\\n@METADATA height_mm 225.0'\n","          '\\n@METADATA distance_mm 450.0'\n","      )\n","    }\n","\n","    # Save ARFF file\n","#     arff_file_path = '/kaggle/working/7/%d/VVB_uniform_%d.arff' %(batch_num,batch_num)\n","    arff_file_path = '/kaggle/working/fid/VVB_uniform_fid.arff' \n","    with open(arff_file_path, 'w') as f:\n","        dump(dataset, f)\n","    n = n + 1"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":3841975,"sourceId":6813978,"sourceType":"datasetVersion"},{"datasetId":3740706,"sourceId":7032264,"sourceType":"datasetVersion"}],"dockerImageVersionId":30559,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":26118.810738,"end_time":"2023-11-23T13:44:12.470406","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-11-23T06:28:53.659668","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}
